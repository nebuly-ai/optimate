<p align="center">
<br><br><br>
<a href="https://nebuly.gitbook.io/nebuly/welcome/quick-start"><img src="https://user-images.githubusercontent.com/83510798/208247207-861541f0-b968-484c-8a0c-0fb110399c16.png" width="400px"></a>
<br><br><br>
</p>

<p align="center">
<b>AI Optimization AppStore to boost the performances of your AI systems</b>
</p>

<p align=center>
<a href="https://pypi.org/project/nebullvm/"><img src="https://badge.fury.io/py/nebullvm.svg"></a>
<a href="https://pypistats.org/packages/nebullvm"><img src="https://pepy.tech/badge/nebullvm"></a>
<a href="https://discord.gg/77d5kGSa8e"><img src="https://img.shields.io/badge/Discord-1.1k-blueviolet?logo=discord&amp;logoColor=white&style=round">
<a href="https://twitter.com/nebuly_ai"><img src="https://img.shields.io/twitter/url/http/shields.io.svg?style=social"></a>

</a>
  


`Nebullvm` is an ecosystem of open-source Apps to boost the performances of your AI systems. The optimization Apps are stack-agnostic and work with any library.

Data. Models. Hardware. These are not independent factors, and making optimal choices on all fronts is hard. Our open source Apps help you to combine these 3 factors seamlessly, thus bringing incredibly fast and efficient AI systems to your fingertips.
Four Apps categories to push the boundaries of AI efficiency. Dozens of Apps.

If you like the idea, give us a star to show your support for the project ⭐

## Accelerate Apps
Achieve sub-10ms response time for any AI application, including generative and language models. Improve customer experience by providing near real-time inferences.


- [x] [Speedster](https://github.com/nebuly-ai/nebullvm/blob/main/apps/accelerate/speedster): Automatically apply SOTA optimization techniques to achieve the maximum inference speed-up on your hardware.
- [x] [Forward-Forward](https://github.com/nebuly-ai/nebullvm/tree/main/apps/accelerate/forward_forward): Test the performance of the Forward-Forward algorithm in PyTorch.
- [ ] [OpenAlphaTensor](https://github.com/nebuly-ai/nebullvm/tree/main/apps/accelerate/open_alpha_tensor): Boost your DL model's performance with OpenAlphaTensor's custom-generated matrix multiplication algorithms (AlphaTensor open-source). 
- [ ] [LargeSpeedster](https://github.com/nebuly-ai/nebullvm/blob/main/apps/accelerate/large_speedster): Automatically apply SOTA optimization techniques on large AI models to achieve the maximum acceleration on your hardware.
- [ ] [CloudSurfer](https://github.com/nebuly-ai/nebullvm/blob/main/apps/accelerate/cloud_surfer): Discover the optimal inference hardware and cloud platform to run an optimized version of your AI model.
- [ ] [OptiMate](https://github.com/nebuly-ai/nebullvm/blob/main/apps/accelerate/optimate): Interactive tool guiding savvy users in achieving the best inference performance out of a given model / hardware setup.

## Maximize Apps
Make your Kubernetes GPU infrastructure efficient. Simplify cluster management, maximize hardware utilization and minimize costs.
- [ ] [GPU Partitioner](https://github.com/nebuly-ai/nebullvm/blob/main/apps/maximize/gpu_partitioner): Effortlessly maximize the utilization of GPU resources in a Kubernetes cluster through real-time dynamic partitioning.
- [ ] [GPUs Elasticity](https://github.com/nebuly-ai/nebullvm/blob/main/apps/maximize/gpus_elasticity): Maximize your GPUs Kubernetes resource utilization with flexible and efficient elastic quotas.

## Extract Apps
Don’t settle on generic AI-models. Extract domain-specific knowledge from large foundational models to create portable, super efficient AI models tailored for your use case.
- [ ] [Promptify](https://github.com/nebuly-ai/nebullvm/blob/main/apps/extract/promptify): Effortlessly fine-tune large language and multi-modal models with minimal data and hardware requirements using p-tuning.
- [ ] [LargeOracle Distillation](https://github.com/nebuly-ai/nebullvm/blob/main/apps/extract/large_oracle_distillation): Leverage advanced knowledge distillation to extract a small and efficient model out of a larger model.

## Simulate Apps
The time for trial and error is over. Simulate the performances of large models on different computing architectures to reduce time-to-market, maximize accuracy and minimize costs.
- [ ] [Simulinf](https://github.com/nebuly-ai/nebullvm/blob/main/apps/simulate/simulinf): Simulate inference performances of your AI model on different hardware and cloud platforms.
- [ ] [TrainingSim](https://github.com/nebuly-ai/nebullvm/blob/main/apps/simulate/training_sim): Easily simulate and optimize the training of large AI models on a distributed infrastructure.


Couldn't find the optimization app you were looking for? Please open an issue or contact us at info@nebuly.ai and we will be happy to develop it together.

---

<p align="center">
  <a href="https://discord.gg/RbeQMu886J">Join the community</a> |
  <a href="https://nebuly.gitbook.io/nebuly/welcome/questions-and-contributions">Contribute to the library</a>
</p>


<p align="center">
<a href="https://nebuly.gitbook.io/nebuly/nebullvm/installation">Installation</a> •
<a href="https://nebuly.gitbook.io/nebuly/nebullvm/get-started">Get started</a> •
<a href="https://github.com/nebuly-ai/nebullvm/tree/main/notebooks">Notebooks</a> •
<a href="https://nebuly.gitbook.io/nebuly/nebullvm/benchmarks">Benchmarks</a>
</p>
